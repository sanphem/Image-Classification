{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clusters Image Classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRngLtKSaFZi"
      },
      "source": [
        "The Clusters dataset is aimed at classifiying 6 images "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIE7VYN3WuMO"
      },
      "source": [
        "Importing the files from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqEw0_WHWghO",
        "outputId": "2bfc063f-1c55-49d0-9679-25421401a3c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neymdR-oXCg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93167d4c-7a2f-43ba-f411-f7c73ed34e0b"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McC8mIuKOo-T"
      },
      "source": [
        "mkdir clusters_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxq-CVWRO3KO",
        "outputId": "b6416427-5823-4044-f4f1-2fbd5b8c3ff2"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mclusters_data\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhn1KP1tO7ZW",
        "outputId": "3f273aa8-357e-48d5-e091-60b62493e8fc"
      },
      "source": [
        "cd clusters_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/clusters_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACPaqKRYPAD4"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUig8YO0Yno4"
      },
      "source": [
        "!cp /content/drive/'My Drive'/'Colab Notebooks'/'Image Classification'/clusters.zip ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkzeUJwDYvQl",
        "outputId": "381054d9-5822-4a23-8742-8b44462b0236"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clusters.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B83xefoKYz8R"
      },
      "source": [
        "# Extracting the files\n",
        "import zipfile\n",
        "path_to_zip_file = 'clusters.zip'\n",
        "directory_to_extract_to =''\n",
        "zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
        "zip_ref.extractall(directory_to_extract_to)\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bjyg9bVZZfp"
      },
      "source": [
        "### remove the zip file\n",
        "!rm clusters.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuaQDoa5ZzFw",
        "outputId": "dc32fbe5-c14b-4b70-f61d-19f56e8ebfb8"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mclusters\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVNuVFvAzNE2"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tku_Si_d--d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269de551-0482-4e6f-f57f-6c0f1d173a82"
      },
      "source": [
        "# load in the training data set\n",
        "\n",
        "train_df = pd.read_csv('/content/clusters_data/clusters/data/train.csv')\n",
        "color_dict = {'red': 0, 'blue': 1, 'green': 2, 'teal':3, 'orange': 4, 'purple': 5}\n",
        "train_df['color'] = train_df.color.apply(lambda x: color_dict[x])\n",
        "\n",
        "#np.random.shuffle(train_df.values)\n",
        "\n",
        "# print(train_df.head())\n",
        "# print(train_df.color.unique())\n",
        "\n",
        "#print(train_df.x.values[0:5])\n",
        "#print(type(train_df.x.values[0:5]))\n",
        "\n",
        "# Build a neural network around this training data.\n",
        "# first thing is to pass a keras sequential type. \n",
        "# The sequetial allows us list the different layers we have in our neural network.\n",
        "# this line helps us access layers. the input shape is 2 cos its just x and y and the output shape is also 2\n",
        "# the 4 is the number of neurons (nodes) in the hidden layers \n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(32, input_shape=(2,), activation='relu'), \n",
        "    #keras.layers.Dropout(0.2), #this drops 20% of the nodes in the hidden layer\n",
        "    keras.layers.Dense(32, activation='relu'), #this adds another hidden layers\n",
        "    keras.layers.Dense(6, activation='sigmoid')])\n",
        "\n",
        "\n",
        "#next is to compile our model i.e to setup loss for our network. this is going to tell us how to train.\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #this updates the network\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# next is to fit the training data into our network\n",
        "\n",
        "#model.fit([x], train_df.color.values, batch_size = 16)\n",
        "# Since [x] is a combination of x and y in the train_df, we have to stack both (pair) columns together using the below code\n",
        "\n",
        "x = np.column_stack((train_df.x.values, train_df.y.values))\n",
        "\n",
        "model.fit([x], train_df.color.values, batch_size = 4, epochs=10)\n",
        "\n",
        "# Evaluate on the test data\n",
        "\n",
        "test_df = pd.read_csv('/content/clusters_data/clusters/data/test.csv')\n",
        "test_x = np.column_stack((test_df.x.values, test_df.y.values))\n",
        "\n",
        "# to evaluate use:\n",
        "\n",
        "print('EVALUATION')\n",
        "\n",
        "test_df['color'] = test_df.color.apply(lambda x: color_dict[x])\n",
        "\n",
        "model.evaluate(test_x, test_df.color.values)\n",
        "\n",
        "print('Prediction =', np.round(model.predict(np.array([[0, 3]]))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9024 - accuracy: 0.6509\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1730 - accuracy: 0.9500\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1185 - accuracy: 0.9706\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1019 - accuracy: 0.9674\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0894 - accuracy: 0.9717\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0804 - accuracy: 0.9717\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0753 - accuracy: 0.9711\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0703 - accuracy: 0.9738\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0670 - accuracy: 0.9757\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0607 - accuracy: 0.9763\n",
            "EVALUATION\n",
            "38/38 [==============================] - 0s 903us/step - loss: 0.0668 - accuracy: 0.9717\n",
            "Prediction = [[0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBqMLZ4h5i5I"
      },
      "source": [
        "## To predict orange, we expect the prediction to be 1 and the 4 position in the array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKnCbCfc4TDb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-trN0zd5Hhk",
        "outputId": "a51cded6-d0b0-4ebe-e858-4f0350070af5"
      },
      "source": [
        "# load in the training data set\n",
        "\n",
        "train_df = pd.read_csv('/content/clusters_data/clusters/data/train.csv')\n",
        "color_dict = {'red': 0, 'blue': 1, 'green': 2, 'teal':3, 'orange': 4, 'purple': 5}\n",
        "train_df['color'] = train_df.color.apply(lambda x: color_dict[x])\n",
        "\n",
        "#np.random.shuffle(train_df.values)\n",
        "\n",
        "# print(train_df.head())\n",
        "# print(train_df.color.unique())\n",
        "\n",
        "#print(train_df.x.values[0:5])\n",
        "#print(type(train_df.x.values[0:5]))\n",
        "\n",
        "# Build a neural network around this training data.\n",
        "# first thing is to pass a keras sequential type. \n",
        "# The sequetial allows us list the different layers we have in our neural network.\n",
        "# this line helps us access layers. the input shape is 2 cos its just x and y and the output shape is also 2\n",
        "# the 4 is the number of neurons (nodes) in the hidden layers \n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(32, input_shape=(2,), activation='relu'), \n",
        "    #keras.layers.Dropout(0.2), #this drops 20% of the nodes in the hidden layer\n",
        "    keras.layers.Dense(32, activation='relu'), #this adds another hidden layers\n",
        "    keras.layers.Dense(6, activation='sigmoid')])\n",
        "\n",
        "\n",
        "#next is to compile our model i.e to setup loss for our network. this is going to tell us how to train.\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #this updates the network\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# next is to fit the training data into our network\n",
        "\n",
        "#model.fit([x], train_df.color.values, batch_size = 16)\n",
        "# Since [x] is a combination of x and y in the train_df, we have to stack both (pair) columns together using the below code\n",
        "\n",
        "x = np.column_stack((train_df.x.values, train_df.y.values))\n",
        "\n",
        "model.fit([x], train_df.color.values, batch_size = 4, epochs=10)\n",
        "\n",
        "# Evaluate on the test data\n",
        "\n",
        "test_df = pd.read_csv('/content/clusters_data/clusters/data/test.csv')\n",
        "test_x = np.column_stack((test_df.x.values, test_df.y.values))\n",
        "\n",
        "# to evaluate use:\n",
        "\n",
        "print('EVALUATION')\n",
        "\n",
        "test_df['color'] = test_df.color.apply(lambda x: color_dict[x])\n",
        "\n",
        "model.evaluate(test_x, test_df.color.values)\n",
        "\n",
        "print('Prediction =', np.round(model.predict(np.array([[-2.5, 3]]))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9745 - accuracy: 0.6072\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1950 - accuracy: 0.9449\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1310 - accuracy: 0.9579\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1050 - accuracy: 0.9670\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0885 - accuracy: 0.9719\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0855 - accuracy: 0.9705\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0764 - accuracy: 0.9723\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0699 - accuracy: 0.9727\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0674 - accuracy: 0.9733\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0700 - accuracy: 0.9717\n",
            "EVALUATION\n",
            "38/38 [==============================] - 0s 904us/step - loss: 0.0754 - accuracy: 0.9642\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd5de42a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Prediction = [[0. 0. 0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n639mLy6Yx6"
      },
      "source": [
        "## To predict blue, we expect the prediction to be ~ 1 and in the 1 position in the array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIiPcnSm6Ek9",
        "outputId": "27b3e9fc-557f-4a00-d279-d1de83cbb656"
      },
      "source": [
        "# load in the training data set\n",
        "\n",
        "train_df = pd.read_csv('/content/clusters_data/clusters/data/train.csv')\n",
        "color_dict = {'red': 0, 'blue': 1, 'green': 2, 'teal':3, 'orange': 4, 'purple': 5}\n",
        "train_df['color'] = train_df.color.apply(lambda x: color_dict[x])\n",
        "\n",
        "#np.random.shuffle(train_df.values)\n",
        "\n",
        "# print(train_df.head())\n",
        "# print(train_df.color.unique())\n",
        "\n",
        "#print(train_df.x.values[0:5])\n",
        "#print(type(train_df.x.values[0:5]))\n",
        "\n",
        "# Build a neural network around this training data.\n",
        "# first thing is to pass a keras sequential type. \n",
        "# The sequetial allows us list the different layers we have in our neural network.\n",
        "# this line helps us access layers. the input shape is 2 cos its just x and y and the output shape is also 2\n",
        "# the 4 is the number of neurons (nodes) in the hidden layers \n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(32, input_shape=(2,), activation='relu'), \n",
        "    #keras.layers.Dropout(0.2), #this drops 20% of the nodes in the hidden layer\n",
        "    keras.layers.Dense(32, activation='relu'), #this adds another hidden layers\n",
        "    keras.layers.Dense(6, activation='sigmoid')])\n",
        "\n",
        "\n",
        "#next is to compile our model i.e to setup loss for our network. this is going to tell us how to train.\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #this updates the network\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# next is to fit the training data into our network\n",
        "\n",
        "#model.fit([x], train_df.color.values, batch_size = 16)\n",
        "# Since [x] is a combination of x and y in the train_df, we have to stack both (pair) columns together using the below code\n",
        "\n",
        "x = np.column_stack((train_df.x.values, train_df.y.values))\n",
        "\n",
        "model.fit([x], train_df.color.values, batch_size = 4, epochs=10)\n",
        "\n",
        "# Evaluate on the test data\n",
        "\n",
        "test_df = pd.read_csv('/content/clusters_data/clusters/data/test.csv')\n",
        "test_x = np.column_stack((test_df.x.values, test_df.y.values))\n",
        "\n",
        "# to evaluate use:\n",
        "\n",
        "print('EVALUATION')\n",
        "\n",
        "test_df['color'] = test_df.color.apply(lambda x: color_dict[x])\n",
        "\n",
        "model.evaluate(test_x, test_df.color.values)\n",
        "\n",
        "print('Prediction =', np.round(model.predict(np.array([[-0.25, 5]]))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.9114 - accuracy: 0.6544\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1710 - accuracy: 0.9510\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1225 - accuracy: 0.9610\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0984 - accuracy: 0.9648\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0913 - accuracy: 0.9662\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0834 - accuracy: 0.9685\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0730 - accuracy: 0.9746\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0739 - accuracy: 0.9687\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0657 - accuracy: 0.9766\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0630 - accuracy: 0.9769\n",
            "EVALUATION\n",
            "38/38 [==============================] - 0s 958us/step - loss: 0.1000 - accuracy: 0.9575\n",
            "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd5d0540510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Prediction = [[0. 1. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4Lw3TTw6_4h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}